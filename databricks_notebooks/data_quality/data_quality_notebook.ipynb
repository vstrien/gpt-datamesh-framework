{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This Data Quality Notebook fetches data quality rules metadata and applies the specified data quality rules (e.g., filter) to the transformed DataFrames. The `apply_data_quality_rule` function checks if the records in the input DataFrame meet the specified conditions and separates valid and invalid records. The resulting valid and invalid records are stored in the `valid_records_dict` and `invalid_records_dict` dictionaries, respectively.\n",
    "\n",
    "You can extend this example to support additional data quality rule types and customizations as needed. Once you have the valid and invalid records, you can decide how to handle them in your pipeline, such as storing invalid records separately for further investigation or cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Import the base notebook with common utility functions\n",
    "# %run /path/to/your/base_notebook\n",
    "\n",
    "# Fetch the metadata for data quality rules\n",
    "data_quality_rules_metadata = fetch_metadata(\"<jdbc_url>\", \"<jdbc_user>\", \"<jdbc_password>\", \"DataQualityRules\")\n",
    "\n",
    "# Assume that you've already created and populated the 'executed_transformations' dictionary from the Data Transformation Notebook\n",
    "# executed_transformations = ...\n",
    "\n",
    "# Function to apply a data quality rule\n",
    "def apply_data_quality_rule(rule_metadata, input_dataframes):\n",
    "    rule_id = rule_metadata[\"RuleID\"]\n",
    "    rule_type = rule_metadata[\"Type\"]\n",
    "    configuration = json.loads(rule_metadata[\"Configuration\"])\n",
    "\n",
    "    input_dataframe = input_dataframes[configuration[\"input_dataset\"]]\n",
    "    output_dataframe = None\n",
    "    valid_records = None\n",
    "\n",
    "    if rule_type == \"filter\":\n",
    "        condition = configuration[\"condition\"]\n",
    "        valid_records = input_dataframe.filter(expr(condition))\n",
    "        output_dataframe = input_dataframe.subtract(valid_records)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported data quality rule type: {rule_type}\")\n",
    "\n",
    "    return rule_id, valid_records, output_dataframe\n",
    "\n",
    "# Apply data quality rules based on the metadata\n",
    "valid_records_dict = {}\n",
    "invalid_records_dict = {}\n",
    "for rule in data_quality_rules_metadata:\n",
    "    rule_id, valid_records, invalid_records = apply_data_quality_rule(rule, executed_transformations)\n",
    "    valid_records_dict[rule_id] = valid_records\n",
    "    invalid_records_dict[rule_id] = invalid_records\n",
    "    log(f\"Applied data quality rule {rule_id}: {valid_records.count()} valid records, {invalid_records.count()} invalid records\")\n",
    "\n",
    "# Now the 'valid_records_dict' and 'invalid_records_dict' dictionaries contain the valid and invalid records based on the data quality rules"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
