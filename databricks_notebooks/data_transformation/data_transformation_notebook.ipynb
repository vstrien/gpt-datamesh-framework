{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Data Transformation Notebook example reads the metadata for transformations and their dependencies, and applies the specified transformations (e.g., filter, select, join, aggregate) to the input DataFrames. The `apply_transformations_recursively` function handles dependencies between transformations, ensuring that they are executed in the correct order. The resulting transformed DataFrames are stored in the `executed_transformations` dictionary.\n",
    "\n",
    "You can extend this example to support additional transformation types and customizations as needed. By implementing the transformations based on metadata, you can create a flexible and extensible Data Transformation Notebook that adapts to various data processing scenarios without having to modify the code.\n",
    "\n",
    "Once you have the transformed DataFrames, you can pass them on to the subsequent stages of your data processing framework, such as data quality checks, data output, and data visualization. This Data Transformation Notebook can be integrated with other components of your data processing framework in Azure Databricks to form a complete, metadata-driven data processing solution.\n",
    "\n",
    "In this Data Transformation Notebook, we've added logging for each transformation applied and stored the processed data in the specified storage path as Parquet files. The file names include a timestamp to ensure uniqueness and to allow for versioning or historical data preservation.\n",
    "\n",
    "With these updates, you'll have a better record of the operations performed, making it easier to audit, debug, and monitor your data processing pipeline. Feel free to modify the storage options, file formats, or logging details as needed for your specific use case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql.functions import col, expr\n",
    "import datetime\n",
    "\n",
    "# Import the base notebook with common utility functions\n",
    "# %run /path/to/your/base_notebook\n",
    "\n",
    "# Fetch the metadata for transformations and transformation dependencies\n",
    "transformations_metadata = fetch_metadata(\"<jdbc_url>\", \"<jdbc_user>\", \"<jdbc_password>\", \"Transformations\")\n",
    "dependencies_metadata = fetch_metadata(\"<jdbc_url>\", \"<jdbc_user>\", \"<jdbc_password>\", \"TransformationDependencies\")\n",
    "\n",
    "# Assume that you've already created and populated a 'dataframes' dictionary from the Data Ingestion Notebook\n",
    "# dataframes = ...\n",
    "\n",
    "# Add your storage path for processed data\n",
    "processed_data_storage_path = \"your_processed_data_storage_path\"\n",
    "\n",
    "# Function to apply a transformation\n",
    "def apply_transformation(transformation_metadata, input_dataframes):\n",
    "    transformation_id = transformation_metadata[\"TransformationID\"]\n",
    "    transformation_type = transformation_metadata[\"Type\"]\n",
    "    configuration = json.loads(transformation_metadata[\"Configuration\"])\n",
    "\n",
    "    input_dataframe = input_dataframes[configuration[\"input_dataset\"]]\n",
    "    output_dataframe = None\n",
    "\n",
    "    if transformation_type == \"filter\":\n",
    "        condition = configuration[\"condition\"]\n",
    "        output_dataframe = input_dataframe.filter(expr(condition))\n",
    "    elif transformation_type == \"select\":\n",
    "        columns = configuration[\"columns\"]\n",
    "        output_dataframe = input_dataframe.select(*[col(c) for c in columns])\n",
    "    elif transformation_type == \"join\":\n",
    "        second_dataframe = input_dataframes[configuration[\"second_dataset\"]]\n",
    "        join_type = configuration[\"join_type\"]\n",
    "        join_condition = expr(configuration[\"condition\"])\n",
    "        output_dataframe = input_dataframe.join(second_dataframe, join_condition, join_type)\n",
    "    elif transformation_type == \"aggregate\":\n",
    "        groupby_columns = configuration[\"groupby_columns\"]\n",
    "        agg_functions = configuration[\"agg_functions\"]\n",
    "        output_dataframe = input_dataframe.groupBy(*[col(c) for c in groupby_columns]).agg(expr(agg_functions))\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported transformation type: {transformation_type}\")\n",
    "\n",
    "    return transformation_id, output_dataframe\n",
    "\n",
    "# Function to recursively apply transformations based on dependencies\n",
    "def apply_transformations_recursively(transformation_id, input_dataframes, executed_transformations):\n",
    "    if transformation_id in executed_transformations:\n",
    "        return executed_transformations[transformation_id]\n",
    "\n",
    "    transformation_metadata = next(t for t in transformations_metadata if t[\"TransformationID\"] == transformation_id)\n",
    "    dependent_transformations = [d[\"DependsOnTransformationID\"] for d in dependencies_metadata if d[\"TransformationID\"] == transformation_id]\n",
    "\n",
    "    for dependent_transformation_id in dependent_transformations:\n",
    "        input_dataframes[dependent_transformation_id] = apply_transformations_recursively(dependent_transformation_id, input_dataframes, executed_transformations)\n",
    "\n",
    "    transformed_dataframe_id, transformed_dataframe = apply_transformation(transformation_metadata, input_dataframes)\n",
    "    executed_transformations[transformed_dataframe_id] = transformed_dataframe\n",
    "    return transformed_dataframe\n",
    "\n",
    "# Apply transformations based on the metadata\n",
    "executed_transformations = {}\n",
    "for transformation in transformations_metadata:\n",
    "    transformation_id = transformation[\"TransformationID\"]\n",
    "    if transformation_id not in executed_transformations:\n",
    "        transformed_dataframe = apply_transformations_recursively(transformation_id, dataframes, executed_transformations)\n",
    "        log(f\"Applied transformation {transformation_id}\")\n",
    "        \n",
    "        # Store the processed data in the specified storage path\n",
    "        output_path = f\"{processed_data_storage_path}/{transformation_id}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        transformed_dataframe.write.parquet(output_path)\n",
    "        log(f\"Stored processed data for transformation {transformation_id} at {output_path}\")\n",
    "\n",
    "# Now the 'executed_transformations' dictionary contains the transformed DataFrames based on the metadata"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
