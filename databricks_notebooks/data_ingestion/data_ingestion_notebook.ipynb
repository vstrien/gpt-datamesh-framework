{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion Notebook\n",
    "\n",
    "This example provides a basic structure for a Data Ingestion Notebook that reads data from various sources based on the metadata and creates Spark DataFrames. You can extend this code to handle additional data source types, formats, and options, as well as integrate it with the rest of your data processing framework in Azure Databricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "import json\n",
    "\n",
    "# Import the base notebook with common utility functions\n",
    "# %run /path/to/your/base_notebook\n",
    "\n",
    "# Fetch the metadata for data sources and datasets\n",
    "data_sources_metadata = fetch_metadata(\"<jdbc_url>\", \"<jdbc_user>\", \"<jdbc_password>\", \"DataSources\")\n",
    "datasets_metadata = fetch_metadata(\"<jdbc_url>\", \"<jdbc_user>\", \"<jdbc_password>\", \"DataSets\")\n",
    "\n",
    "# Initialize the Spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "def read_data(source_type, connection_string, format, options):\n",
    "    if source_type == \"file\":\n",
    "        return spark.read.format(format).options(**options).load(connection_string)\n",
    "    elif source_type == \"database\":\n",
    "        return spark.read.format(format).options(url=connection_string, **options).load()\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported source type: {source_type}\")\n",
    "\n",
    "dataframes = {}\n",
    "\n",
    "# Iterate through the metadata and read the data\n",
    "for ds_meta, d_meta in zip(data_sources_metadata, datasets_metadata):\n",
    "    source_type = ds_meta[\"Type\"]\n",
    "    connection_string = ds_meta[\"ConnectionString\"]\n",
    "    format = ds_meta[\"Format\"]\n",
    "    dataset_name = d_meta[\"Name\"]\n",
    "    schema = json.loads(d_meta[\"Schema\"])\n",
    "\n",
    "    # Set additional options if required (e.g., header, delimiter, etc.)\n",
    "    options = {}\n",
    "\n",
    "    # Read the data from the data source and create a DataFrame\n",
    "    df = read_data(source_type, connection_string, format, options)\n",
    "\n",
    "    # Apply the schema to the DataFrame\n",
    "    df = spark.createDataFrame(df.rdd, schema)\n",
    "\n",
    "    # Add the DataFrame to the dataframes dictionary\n",
    "    dataframes[dataset_name] = df"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
