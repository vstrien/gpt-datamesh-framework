{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this base notebook, we've defined the `fetch_metadata` function that fetches metadata from the specified table in Azure SQL Database and returns it as a list of dictionaries. You can import this base notebook into your other notebooks using the `%run` magic command and use the utility functions across your data processing framework in Azure Databricks.\n",
    "\n",
    "This notebook also includes utility functions for logging (`log`), error handling (`handle_error`), and generating Spark DataFrames based on metadata (`generate_dataframe`). You can import this base notebook into your other notebooks using the `%run` magic command and use these utility functions across your data processing framework in Azure Databricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "import pandas as pd\n",
    "import pyodbc\n",
    "import json\n",
    "import logging\n",
    "\n",
    "# Initialize the Spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Set up logging configuration\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "# Define the function to fetch metadata from Azure SQL Database\n",
    "def fetch_metadata(jdbc_url, jdbc_user, jdbc_password, table_name):\n",
    "    connection_string = f\"DRIVER={{ODBC Driver 17 for SQL Server}};SERVER={jdbc_url};UID={jdbc_user};PWD={jdbc_password}\"\n",
    "    \n",
    "    with pyodbc.connect(connection_string) as conn:\n",
    "        query = f\"SELECT * FROM {table_name}\"\n",
    "        metadata_df = pd.read_sql(query, conn)\n",
    "    \n",
    "    return metadata_df.to_dict(orient=\"records\")\n",
    "\n",
    "\n",
    "# Utility function for logging\n",
    "def log(message, level=logging.INFO):\n",
    "    logging.basicConfig(level=level)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.log(level, message)\n",
    "\n",
    "# Utility function for error handling\n",
    "def handle_error(exception, message=\"An error occured\"):\n",
    "    log(f\"{message}: {str(exception)}\", level=logging.ERROR)\n",
    "\n",
    "# Utility function to generate Spark DataFrame based on metadata\n",
    "def generate_dataframe(data, schema_json):\n",
    "    schema = StructType.fromJson(json.loads(schema_json))\n",
    "    return spark.createDataFrame(data, schema)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example usage of utility functions\n",
    "\n",
    "```python\n",
    "try:\n",
    "    metadata = fetch_metadata(\"<jdbc_url>\", \"<jdbc_user>\", \"<jdbc_password>\", \"DataSources\")\n",
    "    log(\"Fetched metadata successfully\")\n",
    "except Exception as e:\n",
    "    handle_error(e, \"Failed to fetch metadata\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
